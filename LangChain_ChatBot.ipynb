{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOh9baw+nNSiy3E+qc3PQJC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IshaSarangi/LangChain_Chatbot/blob/main/LangChain_ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Building A Chatbot\n",
        "\n",
        "We will go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions.\n",
        "\n",
        "Note that this chatbot that we build will only use the language model to have a conversation. There are several other related concepts that you may be looking for:\n",
        "\n",
        "*   Conversational RAG: Enable a chatbot experience over an external source of data\n",
        "*   Agents: Build a chatbot that can take actions\n",
        "\n"
      ],
      "metadata": {
        "id": "EkkJGZqAMKob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()   #Loading all the environment variable\n",
        "from google.colab import userdata\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')\n",
        "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
      ],
      "metadata": {
        "id": "tDgBPnl6Mx5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgCHNGceNJ3J",
        "outputId": "3d804edc-6eb6-4244-c02f-e2e82285de5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_groq in /usr/local/lib/python3.12/dist-packages (0.3.8)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain_groq) (0.3.78)\n",
            "Requirement already satisfied: groq<1,>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from langchain_groq) (0.32.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain_groq) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain_groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain_groq) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain_groq) (4.15.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.75->langchain_groq) (0.4.33)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.75->langchain_groq) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.75->langchain_groq) (1.33)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.75->langchain_groq) (6.0.3)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.75->langchain_groq) (25.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.30.0->langchain_groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain_groq) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain_groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain_groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.75->langchain_groq) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain_groq) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain_groq) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain_groq) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain_groq) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain_groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain_groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain_groq) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain_groq) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.75->langchain_groq) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "# model = ChatGroq(model='Gemma2-9b-It', groq_api_key=groq_api_key)\n",
        "model = ChatGroq(model='llama-3.1-8b-instant', groq_api_key=groq_api_key)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUu2LgtXNR3Q",
        "outputId": "12de2626-bccd-4863-cfe2-6bd29ef79852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7de0d02e6270>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7de0acfaef60>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "model.invoke([HumanMessage(content=\"Hi, My name is Isha and I am an Gen AI enthusiast. I live in Cuttack, Odisha\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b3_6PAsNd41",
        "outputId": "c36ab793-ad94-4b65-b24b-8030c872d5e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Nice to meet you, Isha. It's great to hear that you're a Gen AI enthusiast. Odisha is a beautiful state with rich cultural heritage, and Cuttack is a historic city with a lot of interesting stories to tell.\\n\\nWhat aspects of Gen AI do you find most fascinating? Are you interested in its potential applications, its technological advancements, or perhaps its impact on society? And do you have any favorite AI-related projects or developments that you've come across recently?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 59, 'total_tokens': 157, 'completion_time': 0.183608952, 'prompt_time': 0.00391552, 'queue_time': 0.215016658, 'total_time': 0.187524472}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_e32974efee', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--cab058d8-55de-4a2c-b340-585369dfc331-0', usage_metadata={'input_tokens': 59, 'output_tokens': 98, 'total_tokens': 157})"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage\n",
        "model.invoke(\n",
        "    [\n",
        "        HumanMessage(content=\"Hi, My name is Isha and I am an Gen AI enthusiast. I live in Cuttack, Odisha\"),\n",
        "        AIMessage(content=\"Namaste Isha! It's great to meet you. I'm happy to learn that you're an enthusiast of Gen AI. Cuttack, the silver city of Odisha, is a beautiful place to live. What sparked your interest in General Artificial Intelligence (Gen AI), and what do you hope to achieve or learn about it?\"),\n",
        "        #HumanMessage(content=\"Hey What's my name and what do I do?\")\n",
        "        HumanMessage(content=\"Which city is known as the silver city of Odisha?\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKQsZJb4Pd1Q",
        "outputId": "c72b3829-f8bd-4ca8-de62-9f993c8aa329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Cuttack is actually known as \"The Millennium City\" or \"The Silver City of Odisha\". The city\\'s name is often associated with its beautiful silver filigree work, which is a famous tradition in Odisha. The city has a rich history and is a significant cultural hub in the state.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 63, 'prompt_tokens': 149, 'total_tokens': 212, 'completion_time': 0.117899316, 'prompt_time': 0.008420354, 'queue_time': 0.2228159, 'total_time': 0.12631967}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_7b3cfae3af', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--2de56fa1-9b91-48a1-8f79-2605e75d20bd-0', usage_metadata={'input_tokens': 149, 'output_tokens': 63, 'total_tokens': 212})"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Message History\n",
        "\n",
        "We can use a message history class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input."
      ],
      "metadata": {
        "id": "iPYXPUdmQHRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stLhLhZJQeFE",
        "outputId": "1120d128-3a5f-4bda-d6bd-5cea2623a9f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.12/dist-packages (0.3.31)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.78 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.3.78)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.43)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.11.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.33)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.78->langchain_community) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.78->langchain_community) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.78->langchain_community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=0.3.78->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain_community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "store = {}\n",
        "\n",
        "def get_session_history(session_id:str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "with_message_history = RunnableWithMessageHistory(model, get_session_history)"
      ],
      "metadata": {
        "id": "X7RAuUhjQiPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {'configurable': {'session_id': 'chat1'}}"
      ],
      "metadata": {
        "id": "-zxYOClPSG3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = with_message_history.invoke(\n",
        "    [HumanMessage(content=\"Hi, My name is Isha and I am an Gen AI enthusiast. I live in Cuttack, Odisha\")],\n",
        "    config=config\n",
        ")"
      ],
      "metadata": {
        "id": "dWgm1AHNSNrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "rM8F9q72Sgk6",
        "outputId": "700162c9-18e2-4ef9-8ba3-f73d3a190a0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Nice to meet you, Isha. It's great to hear that you're a Gen AI enthusiast. Odisha is a beautiful state with a rich cultural heritage. Cuttack, being the former capital of Odisha, has a lot of historical significance. \\n\\nHow did you get interested in Gen AI, and what aspects of it fascinate you the most?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with_message_history.invoke(\n",
        "    [HumanMessage(content=\"What is my name?\")],\n",
        "    config=config\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXgYW5nxSiis",
        "outputId": "6c13fb6a-957c-44b5-ea84-c01e20eec01f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is Isha.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 147, 'total_tokens': 154, 'completion_time': 0.011141849, 'prompt_time': 0.010978181, 'queue_time': 0.356085964, 'total_time': 0.02212003}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_7b3cfae3af', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--eac3a63c-a5d6-44b4-aa47-ca58c9c25149-0', usage_metadata={'input_tokens': 147, 'output_tokens': 7, 'total_tokens': 154})"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#change the config-->session id\n",
        "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
        "response=with_message_history.invoke(\n",
        "    [HumanMessage(content=\"Whats my name\")],\n",
        "    config=config1\n",
        ")\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "bXyGIdP7WEPM",
        "outputId": "4f38197c-ce01-4c15-ca19-4b803bdb56ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I don't have any information about your name. This is the beginning of our conversation, and I don't have any prior knowledge about you. If you'd like to share your name with me, I'd be happy to know it.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=with_message_history.invoke(\n",
        "    [HumanMessage(content=\"Hey My name is Isha\")],\n",
        "    config=config1\n",
        ")\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "19_0SnEwWKLY",
        "outputId": "166972f5-a1af-42c5-c000-d1991e503039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Nice to meet you, Isha. How are you doing today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=with_message_history.invoke(\n",
        "    [HumanMessage(content=\"Whats my name\")],\n",
        "    config=config1\n",
        ")\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "OkBxwYs3WT21",
        "outputId": "d1f9ec9b-c5f4-47f2-da42-c289df3a6723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is Isha. I remember from earlier in our conversation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prompt Templates\n",
        "\n",
        "Prompt Templates help to run raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we will add in more input besides just the messages."
      ],
      "metadata": {
        "id": "ZVHmY3BSWZ3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        ('system', 'You are a helpful assisstant. Answer all questions to the best of your ability.'),\n",
        "        MessagesPlaceholder(variable_name='messages')\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt|model"
      ],
      "metadata": {
        "id": "m1Zyy3OxW0Q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({'messages':[HumanMessage(content='Hi. My name is Isha.')]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csmRb396XZgb",
        "outputId": "65484ce2-ea51-4dd2-e273-36dc0a449dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Nice to meet you, Isha. I'm happy to assist you with any questions or topics you'd like to discuss. Is there something specific on your mind, or would you like to start with a conversation?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 61, 'total_tokens': 105, 'completion_time': 0.072413509, 'prompt_time': 0.004426732, 'queue_time': 0.293362253, 'total_time': 0.076840241}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--b14609a0-6741-49bf-8baf-f86d1964c642-0', usage_metadata={'input_tokens': 61, 'output_tokens': 44, 'total_tokens': 105})"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with_message_history = RunnableWithMessageHistory(chain, get_session_history)"
      ],
      "metadata": {
        "id": "JqBp_TuCXjCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {'configurable': {'session_id': 'chat3'}}\n",
        "response = with_message_history.invoke(\n",
        "    [HumanMessage(content='Hi. My name is Isha.')],\n",
        "    config=config\n",
        ")\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxm68TPaXqQg",
        "outputId": "fd76aed6-b96e-41dd-ce41-a737065f8463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello Isha, it's nice to meet you. I'm here to assist you with any questions or topics you'd like to discuss. How can I help you today?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 61, 'total_tokens': 97, 'completion_time': 0.041938639, 'prompt_time': 0.003746845, 'queue_time': 0.215219354, 'total_time': 0.045685484}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_33e8adf159', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--fcaf9947-3243-4cfd-821e-8a6064df33ed-0', usage_metadata={'input_tokens': 61, 'output_tokens': 36, 'total_tokens': 97})"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = with_message_history.invoke(\n",
        "    [HumanMessage(content=\"What's my name?\")],\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "DJICKKVvYCU5",
        "outputId": "3f168fe5-bc8f-4a24-c6c5-8bb6b5761323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is Isha.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Add more complexity\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            'system',\n",
        "            'You are a helpful assisstant. Answer all questions to the best of your ability. '\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name='messages'),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt|model"
      ],
      "metadata": {
        "id": "BfuaKf69YFSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke({'messages': [HumanMessage(content='Hi. My name is Isha.')]})\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "4hp5HQ0iYaF-",
        "outputId": "28e2c91d-32d7-4f0c-e746-a97678ac2b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello Isha, it's nice to meet you. I'm here to help with any questions or topics you'd like to discuss. How's your day going so far?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now wrap this more complicated chain in a Message History Class. This time, because there are multiple keys in the input, we need to specify the correct key to use to save the chat history"
      ],
      "metadata": {
        "id": "ikRG629DYjBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key = 'messages'\n",
        ")"
      ],
      "metadata": {
        "id": "1A08NT4uYvwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"session_id\": \"chat4\"}}\n",
        "repsonse=with_message_history.invoke(\n",
        "    {'messages': [HumanMessage(content=\"Hi, I am Isha\")],\"language\":\"Hindi\"},\n",
        "    config=config\n",
        ")\n",
        "repsonse.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "geCgFNkrY5L9",
        "outputId": "7a5e7228-4bc2-4e62-e8ba-e8ac51e16c54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Nice to meet you, Isha. How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = with_message_history.invoke(\n",
        "    {'messages': [HumanMessage(content=\"What is my name?\")], \"language\": \"Hindi\"},\n",
        "    config=config,\n",
        ")\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8llxm61uZibr",
        "outputId": "8f4f388f-5451-490a-f198-fcc8aa01bb0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is Isha.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Managing the Conversation History\n",
        "\n",
        "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in. \"trim_messages\" helps to reduce how many messages we are sending to the model. The trimmer allows us to specify how many tokens we want to kepp, along with other parameters like if we want to always keep the system message and whether to allow partial messages."
      ],
      "metadata": {
        "id": "CXveoMWxZ0V0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage,trim_messages\n",
        "trimmer=trim_messages(\n",
        "    max_tokens=500,\n",
        "    strategy=\"last\",\n",
        "    token_counter=model,\n",
        "    include_system=True,\n",
        "    allow_partial=False,\n",
        "    start_on=\"human\"\n",
        ")\n",
        "messages = [\n",
        "    SystemMessage(content=\"you're a good assistant\"),\n",
        "    HumanMessage(content=\"hi! I'm Isha\"),\n",
        "    AIMessage(content=\"hi!\"),\n",
        "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
        "    AIMessage(content=\"nice\"),\n",
        "    HumanMessage(content=\"whats 2 + 2\"),\n",
        "    AIMessage(content=\"4\"),\n",
        "    HumanMessage(content=\"thanks\"),\n",
        "    AIMessage(content=\"no problem!\"),\n",
        "    HumanMessage(content=\"having fun?\"),\n",
        "    AIMessage(content=\"yes!\"),\n",
        "]\n",
        "trimmer.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E47CQ46cacbO",
        "outputId": "1b27e6e0-a229-4fa9-abbc-ae2d7d764fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"hi! I'm Isha\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(messages=itemgetter('messages')|trimmer)\n",
        "    | prompt\n",
        "    | model\n",
        ")\n",
        "\n",
        "response=chain.invoke(\n",
        "    {\n",
        "    \"messages\":messages + [HumanMessage(content=\"What ice cream do I like?\")],\n",
        "    \"language\":\"English\"\n",
        "    }\n",
        ")\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "leZ83ZONaiEd",
        "outputId": "a39046b8-a8bf-4479-8cdb-4a085b93cfb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You like vanilla ice cream!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke(\n",
        "    {\n",
        "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
        "        \"language\": \"English\",\n",
        "    }\n",
        ")\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9-PY4-2HbFJF",
        "outputId": "58061cc2-6c65-493a-f684-1446ad89869d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You asked the math problem \"what\\'s 2 + 2\".'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets wrap this in the Message History\n",
        "with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"messages\",\n",
        ")\n",
        "config={\"configurable\":{\"session_id\":\"chat5\"}}"
      ],
      "metadata": {
        "id": "90sksy5wbMRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = with_message_history.invoke(\n",
        "    {\n",
        "        \"messages\": messages + [HumanMessage(content=\"Whats my name?\")],\n",
        "        \"language\": \"English\",\n",
        "    },\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "uzzKxGd-bPTe",
        "outputId": "98b2d9d4-cb0d-4036-b546-170129b09cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Isha!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = with_message_history.invoke(\n",
        "    {\n",
        "        \"messages\": [HumanMessage(content=\"What math problem did I ask?\")],\n",
        "        \"language\": \"English\",\n",
        "    },\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "G_hWiz9UbQha",
        "outputId": "3b546727-c546-4746-f7bd-fbf9c2513e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You asked the math problem: 2 + 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    }
  ]
}